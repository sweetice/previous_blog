---
layout:     post
title:      How to distinguish between on-policy learning and off-policy learning ?
subtitle:   如何区分在线学习和离线学习
date:       2018-10-09
author:     Johnny
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Reinforcement Learning
---

# 如何区分on-policy 和 off-policy
  本文讲如何区分强化学习中的在线学习和离线学习。

## 典型的on-policy 和 off-policy 策略

Q-learning 是典型的off-policy 策略，其更新规则为：
	
$$
Q\left(S,A\right) <- Q\left(S,A\right) + \alpha\left[R+\gamma\left(max\left(Q\left(S^',A\right)-Q(\left(S,A\right)\right]
$$

Sarsa 是典型的on-policy策略，其更新规则为：

$$
Q\left(S,A\right) <- Q\left(S,A\right) + \alpha\left[R+\gamma\left(Q\left(S^',A^'\right)-Q\left(S,A\right)\right]
$$


OK，这好像也一些绕。

我们直接看更新规则！

在更新规则中，Q-learning出现了max，max意味着我调用了历史经验，使用了以前学习到的东西

而SARSA中，我只是使用了state-action-reward-nextstate-nextaction，我这里没有使用以前的经验。

回到问题上来，如何区分？？？

答案呼之欲出：使用了历史经验，就是off-policy。不使用，那就是on-policy。







