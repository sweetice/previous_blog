---
layout:     post
title:      强化学习模型不稳定原因的归纳
subtitle:   为什么强化学习不稳定呢？
date:       2018-11-27
author:     Johnny
header-img: img/post-bg-reinforcement-learning.png
catalog: true
tags:
    - Reinforcement Learning
---

# 强化学习模型不稳定的猜测

我们在训练强化学习模型的时候可能经常遇到一种情况：

*我的模型已经训练好了，从reward来看，已经收敛了，我就停下来，然后把模型保存好，下一次接着用。但是我下一次用的时候，发现数据就混乱了，reward根本达不到我的预期。*

这种情况在强化学习中非常普遍。在CV领域也会遇到，但是频率相对较低。

## 前置知识：神经网路的不稳定性

笔者在训练网络的时候经常在思考，高维空间（常常是百万数量级）的loss曲面分布到底是什么样的？

这个问题非常有意义，如果loss曲面到处都是local minimum，那么根据神经网络的经典理论，其实并不会有多大的印象，我们的网络该收敛就收敛。但是真的是这样子吗？

*高度对称的loss曲面，如果到处都是鞍点呢？*

好像也没啥影响，在我有噪声影响的情况下，使用梯度下降方法，还是很容易走出鞍点的。

*高度对称的loss曲面，鞍点附近都是平缓的区域呢？*

这个时候情况就非常糟糕了！！！

此时我们的即使走出了鞍点，随之而来就是平缓区域，平缓意味着梯度非常非常小（某种程度上你可以认为是0）。

这样子的话，优化的方向就是在这个平缓的区域里面随机游走。

当loss足够小时，一段时间后，做实验的你看到网络性能不再上升，就觉得收敛了（通常意义下确实如此）。然后高兴地报告你的老师实验做好了，性能提升了多少多少个百分点。




## 猜想之一：新数据的分布和老数据不一样

比起CV领域，我的数据是一直保存下来的，存在硬盘里的，强化学习的数据是靠agent和environment不停互动而产生。数据本身就有[不稳定性](https://arxiv.org/abs/1611.05763).

这样子，当我们在这一串tensor数据中训练到稳定。这意味着我们在前一节说的，此时loss陷入了一个平缓的，值又足够的小时，那么你可能就认为网络收敛了；于是你很高兴保存了模型，准备下一次用。

*但，真的是这样吗？？？*

中午在食堂吃了好吃的水煮鱼，你今天很高兴，决定偷个懒，用上一次的模型应付这一次老师给你布置的写新模型的任务。于是睡了午觉后，你打开了自己上次保存的模型。

你看着模型在run，好像reward曲线不太对。。。emmm，模型gg了。

发生了什么？

新的环境和旧的agent interact，然后产生了一个和原来loss曲面分布完全不一样的新的分布，于是你的模型在这个分布上不停地run，无法进入原来收敛到的鞍点附近的平缓区域。

这样子模型就坏掉了。

你就有一个疑惑，你说的数据分布，loss曲面我全都不懂，我就觉得我的模型训练好的，环境还是原来的环境。收敛的模型也是在原来的环境里面interact，为什么就坏掉了呢？

这个我们下次再讲。
## 未完待续...

