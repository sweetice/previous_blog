---
layout:     post
title:      Methods for training RL in sparse reward tasks
subtitle:   在稀疏奖励环境下做强化学习的方法
date:       2018-11-19
author:     Johnny
header-img: img/post-bg-reinforcement-learning.png
catalog: true
tags:
    - Reinforcement Learning
---


强化学习有一个方向是专注于解决稀疏奖励中的策略问题。这是目前研究的一个热点。

相信大家对于强化学习已经有了一定的概念，其中Model-Free的强化学习方法更是获得很大的成功，并在一些任务上达到了与人类专家媲美的效果。利用当前某一时刻的信息预测未来的结果与趋势。这种方法主要由两个流程组成：

1.  收集数据：使用当前的策略模型与环境进行交互，并获得一定量的交互结果：<状态s，行动a，奖励r>等数据，通常来说，这些数据包含着一定的信息：要么这些状态行动序列最终会带来好的结果，要么带来不好的结果。
2.  训练模型：利用收集到的数据训练模型，使模型能够“记忆”这些数据，增加能带来好的结果的概率，同时减少带来坏的结果的概率。同时我们希望在训练中模型能够拥有一定的泛化性，即当面对类似的状态时可以得出类似的结果。

从上面的流程可以看出，Model-Free的方法其实和监督学习方法有一些相近之处，第一步通过某种方法挑选出一些有价值的数据，第二步利用这些数据进行训练，使其达到某种特定的目标。利用DQN算法，我们希望模型能够拟合得到当前状态行动下的长期打折回报。这种方法假设Agent并不知晓环境的运转方法，所以他更加专注于“拟合”这件事。但是，Model-Free的方法还是存在一定的问题：

1.  在训练Model-Free时，我们需要收集大量的训练数据，这个数量和问题规模相关，越是规模大的问题，它需要的数量越大。为了减小数据对模型带来的不稳定性，数据量不能够太小；
2.  由于模型只是机械地记忆一些结果，因此我们无法保证模型在学习过程中是否真的学到了一些有用的内容，又或者真的只是完成了记忆的工作，模型在应对类似的问题时就会出现问题。比方说对于一个迷宫问题，我们用一个模型学习了某个特定结构的迷宫后，当一个结构不同的迷宫出现时，它是否同样能够给出较好的表现，这个时存在一些挑战的。
3.  对于一些奖励稀疏的问题来说，如果奖励是稀疏的，那么收集到的数据很可能并不能表现出自己的结果是好是坏，那么这些数据无法帮助模型带来很大的提升，那么模型的学习就会变得比较复杂。

对于上面的这些问题，研究人员已经提出了很多办法，这些办法可以缓解其中的部分问题。解下来我们就来看看其中的第三个问题：奖励稀疏问题，以及近期一些关于解决这个问题的研究成果。

## **稀疏奖励问题**

相信大家都已经对这个游戏有了一定的了解：Montezuma's Revenge。这个游戏就是奖励稀疏的一个经典例子，在这个游戏中，Agent只可以通过获取钥匙和打开房间门或者奖励，而触碰了骷髅头之后会扣除一条生命，除此之外，其他的操作对于Agent来说将不会获得任何的奖励与惩罚，这样绝大多数的操作将无法对Agent产生影响，Agent在获得第一个奖励之前将无法获得任何正向的反馈，因此模型很可能会停止学习而无法提升。

为了解决这个问题，一个最简单的方法就是修改奖励函数。原来的奖励函数不是一个稀疏的函数么？那把它替换成一个不稀疏的奖励函数是不是就可以了？这个想法是没有问题的，同时通过修改奖励函数也会得到不错的结果，但是这种方法也存在一些问题：这样精心设计的函数需要专家来完成，这些同样是解决问题需要的成本。同时这也与构建一个通用人工智能模型的愿景存在一定分歧，我们更希望模型能够自主学习，而不是有专家参与才能完成这样的任务。当然在短时间内实现这样的功能有点不切实际，至少我们希望专家的工作能够劲可能地减少。

总体来说，目前解决这个问题可以采用如下的一些方法：

*   结合专家操作序列
*   课程学习
*   自我博弈
*   层次化强化学习
*   基于计数的探索方法

下面我们就来看看这些方法。

## **结合专家操作序列**

相较而言，让专家设计一个奖励函数的难度相对较大，而让专家示范如何在任务中取得更好的成绩就显得更简单一些。比方说对于自动驾驶来说，回答“怎样驾驶是对的”这样的问题是有一定难度的，但是亲自示范正确的驾驶行为则显得容易了不少。那么我们能不能利用专家给出的一些示范来帮助模型快速掌握一些行动方法，从而减轻稀疏奖励函数带来的影响。

具体来说，对于一个Off-Policy的方法，我们可以使用Replay Buffer保存一些待训练的数据，其中一部分来源于Agent与环境的交互，一部分来源于专家的示范，模型将结合两部分数据进行训练。

## **课程学习**

课程学习是一个让Agent"循序渐进"完成学习的方法。有时一个如同一张白纸的Agent面对的问题，是一个十分复杂且困难的问题，就好比让一个婴儿学习微积分一样，这样直接学习很难取得很好的效果。于是我们需要简化问题的难度，先给Agent出一些相对简单的问题，等它逐渐适应了这样的难度后，再提出一些更难的问题，通过这样的方法，模型就能够更快地适应并取得更好的效果。

首先要介绍的方法被称为反向课程学习。反向课程学习与课程学习的思想类似，但是方向正好相反。同样时前面介绍的让婴儿学习微积分的问题，对于课程学习来说，我们的第一步是让婴儿学习认数，然后开始学习加减法，再不断地学习新的内容，让婴儿的能力不断地靠近微积分的水平；而对于反向课程学习来说，它的第一步则是反了过来，我们直接给出求解积分的倒数第二步，然后告诉婴儿在这种情况下怎样一步得出最终的结果，等婴儿理解了这一步，再将问题改到倒数第三步，然后让婴儿向前推导两步，从而得到最终的结果。这样的方法听上去有些不可思议，但实际上这也是一种很有效的方法。它可以让婴儿一直关注结果，同时也可以获得较好的学习效果。

这样我们的问题就变成了——如何选择更合适的“起点”，使得像婴儿一样的模型能够快速成长。我们可以用当前Agent模型在这个点上的长期回报来衡量这个点的合适程度。我们希望Agent模型不会获得太低的得分——这样说明这个起点对Agent有点困难了，同时也希望Agent不会获得太高的得分——这样说明这个起点有点简单。

除了上面提到的设定中间起点方法，我们还可以采用其他的方法，前面提到了专家示范，我们同样可以使用专家示范来帮助我们找到反向课程学习的起点。对于有些问题来说，直接使用终点状态附近的状态不一定会有好的结果，这在前面已经分析过了，而且策略模型本身处在一个变化的状态中，有时判断也会难免有误，同时判断的过程也要耗费一定的时间。如果又了专家示范的操作，我们可以直接从专家示范操作的序列中选择合适的起点作为反向课程学习的起点。

## **自我博弈**

说起自我博弈，大家一定会想到AlphaZero，这个只通过自我博弈就可以掌握围棋的博弈系统。由此可见，自我博弈确实是一种让Agent获得成长的办法。自我博弈的精髓在于两个旗鼓相当的对手可以通过对弈获得提升自身能力的宝贵经验，如果没有这样的对弈环境，让一个人凭空掌握围棋的精妙，恐怕就成了一件几乎不可能的事情。

于是有人基于这个思想，开始了一些尝试。他们利用现有单一Agent环境构建一个对弈环境。比方说本来我们希望训练一个机器人学会站立，学会简单的行走，现在就可以把环境改为两个Agent对抗的环境，在这个新的环境中，两个Agent可以相互推挤，让对方站不稳，这样Agent就可以训练在一定外力下仍然保持平衡的能力。

自我博弈虽然是一个不错的学习方法，但是它也存在着一些风险：如果Agent掌握了对手的情况，就更容易采用一些固定的套路针对对手，来让自己取得成功，这样的胜利显然不是我们想要的，因为这种胜利方式很可能只对特定的对手有效，而对于更多的对手来说，这个方法可能不会起作用。这个现象和GAN里面的"Mode Collapse"也非常相似，生成模型找到了欺骗判别模型的方法之后，就容易“不思进取”。为了解决这个问题，我们需要准备一个“对手池”，每次从对手池中随机选择一个“能力差距不大”的对手进行对弈，这样可以更好地确保模型不会过拟合。

## **分层强化学习**

层次强化学习一般将问题分成了两个层次，一个层次表示了当前任务的状态和行动，另一个层次表示了更高层次的状态和行动（我们称之为goal）。在一些问题中这个更高层次的状态和原本层次的状态类似，比方说它们可以表示一些“中间目标”。这样完整的Agent系统将主要分成两部分：

*   Meta Controller：一个更宏观的策略模型，它会基于当前的状态生成“小目标”，然后将小目标传到下级
*   Controller：一个更具体的策略模型，它会基于Meta Controller生成的“小目标”和当前的状态，得到真正实际的行动。

这就好比如果我们从某个起点状态到达终点状态，Meta Controller则是将整个过程分成了几个部分，而Controller则分别完成每一个小部分的工作。

今年南京大学俞扬团队出了一篇星际争霸环境中的强化学习文章，他们打败了目前星际争霸2中难度最高的AI，其中一个关键的技术就是分层强化学习。俞扬团队做了大量hard-code 的工作。他们的goal是直接人工编制的。我认为他们方向走偏了。

goal是什么？这是一个目前等待探索的问题。我认为goal是一张feature map。

## **基于计数的探索**

基于计数的探索方法起初被应用在有限状态的问题上，因为状态数量是有限的，所以我们可以记录每一个状态出现的次数，于是我们就可以使用一个新的回报函数来表示：



这个公式的第二项中的​是一个常量，​表示为状态s被访问过的次数，显然一个状态被访问的次数次数，继续探索它来带的新鲜感就会越低，因此这部分的得分也会更低。反之则越高。

在状态有限的问题中，这样的公式是轻松实现，但是对于一些更为复杂的问题，状态数量多到没法表示时，直接使用这个公式就会显得有限不适合。于是我们需要一个映射函数将状态映射到一个有限的状态空间中，这样才能方便计算。那么该怎么实现这个方法呢？我们希望相近的状态可以映射到相同的数值中，这样计算的结果会更加合理。
